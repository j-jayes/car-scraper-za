---
title: "ML modelling"
format: html
---

```{r}
library(tidyverse)

setwd(here::here())
```

```{r}
df <- read_rds("clean_data/df_complete_2022-12-27-02-53-pm.rds")
```


### EDA

```{r}
plot_price_cts <- function(var){
  
  df %>% 
    slice_sample(n = 10000) %>% 
    ggplot(aes({{ var }}, price)) +
    geom_smooth() +
    geom_hline(yintercept = 0, lty = 2) +
    scale_y_continuous(labels = scales::dollar_format(prefix = "R"))
    # labs(title = glue::glue("Relationship between {var_lab} and price"))
  
}

plot_price_cts(var = year)

plot_price_cts(var = kilometers)
```


```{r}
plot_price_factor <- function(.data, var){
  .data %>% 
    mutate(across({{ var }}, .fns = ~ fct_lump(., 7)),
           across({{ var }}, .fns = ~ fct_reorder(., price, median))) %>% 
    ggplot(aes(price, {{ var }}, fill = {{ var }})) +
    geom_boxplot() +
    scale_x_continuous(labels = scales::dollar_format(prefix = "R"))
  
}

df %>% 
  slice_sample(n = 2000) %>% 
  plot_price_factor(fuel_type)

df %>% slice_sample(n = 2000) %>% 
  plot_price_factor(make_model)
```


### Models

```{r}
library(tidymodels)

df_small <- df %>% 
  drop_na() %>% 
  select(-c(text, ad_url)) %>% 
  mutate(price = log(price))

split <- initial_split(df_small)

df_train <- training(split)
df_test <- testing(split)

df_folds <- vfold_cv(df_train, v = 5)
```

```{r}
df_small %>% 
  count(make_model, sort = T)
```



```{r}
svm_rec <- recipe(price ~ ., data = df_train) %>%
  update_role(title, new_role = "id") %>%
  step_other(where(is.factor), threshold = .01) %>%
  step_zv(all_predictors()) %>%
  step_novel(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors(), one_hot = T) %>%
  step_normalize(all_numeric_predictors())

# svm_rec %>% prep() %>% juice() %>% view()
```

```{r}
# svm_rec %>% 
#   prep() %>% 
#   juice()
```

```{r}
log_grid <- grid_regular(penalty(), levels = 5)
```


```{r}
log_spec <- linear_reg(engine = "glmnet", mode = "regression", penalty = tune(), mixture = 0)

wf_log <- workflow(svm_rec, log_spec)

log_tune <- tune_grid(object = wf_log, resamples = df_folds, grid = log_grid)

log_tune %>% 
  collect_metrics() %>% 
  ggplot(aes(factor(penalty), mean, fill = .metric)) +
  geom_col() +
  facet_wrap(~ .metric)
```

```{r}
log_wf <- finalize_workflow(wf_log, select_best(log_tune, "rsq"))



log_pred <- last_fit(log_wf, split)
  
log_pred %>% 
  collect_metrics()

log_pred %>% 
  collect_predictions() %>% 
  slice_sample(n = 5000) %>% 
  ggplot(aes(x = price, y = .pred)) +
  geom_point(alpha = .3, colour = "midnightblue") +
  geom_abline()
```

#### What is important??

```{r}
library(vip)

log_pred %>%
  extract_fit_parsnip() %>%
  vi() %>%
  mutate(
    Importance = abs(Importance),
    Variable = fct_reorder(Variable, Importance)
  ) %>%
  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)
```

### Export model

```{r}
final_lasso_model <- fit(log_wf, df_small)

# setwd(here::here())

final_lasso_model %>% write_rds("models/final_lasso_model.rds", compress = "gz")
```





### linear reg

```{r}
lin_reg_spec <- linear_reg()

lin_reg_wf <- workflow(svm_rec, lin_reg_spec)

fit <- fit_resamples(lin_reg_wf, resamples = df_small_folds, metrics = metric_set(rmse, rsq))

fit %>% 
  collect_metrics()
```


```{r}
final_fit <- fit(lin_reg_wf, df_small)

final_fit %>%
  extract_fit_parsnip() %>%
  tidy() %>%
  filter(term != "(Intercept)") %>%
  mutate(cat = case_when(
    str_detect(term, "make_model") ~ "Make & model",
    str_detect(term, "colour") ~ "Colour",
    str_detect(term, "province") ~ "Province",
    TRUE ~ "Numeric"
  )) %>%
  mutate(term = str_remove(term, "make_model|colour|province")) %>%
  mutate(term = fct_reorder(term, estimate)) %>%
  ggplot(aes(estimate, term, fill = estimate > 0)) +
  geom_col() +
  facet_wrap(~cat, scales = "free_y")
```


### Text recipe

```{r}
library(textrecipes)

df_text <- df %>% 
  select(text, price) %>% 
  drop_na() %>% 
  slice_sample(n = 20000)

text_rec <- recipe(price ~ ., data = df_text) %>%
  step_mutate(text = str_to_lower(text)) %>% 
  step_text_normalization(text) %>%
  step_tokenize(text, token = "words") %>% 
  # step_stopwords(text) %>% 
  step_tokenfilter(text) %>% 
  step_tf(text)

text_rec %>%
  prep() %>%
  juice()

text_spec <- svm_linear(mode = "regression", engine = "LiblineaR")

text_wf <- workflow(text_rec, 
                    text_spec)

text_fit <- fit(text_wf, df_text)
```



```{r}
text_fit %>% 
  extract_fit_parsnip() %>% 
  tidy() %>% 
  mutate(term = str_remove(term, "tfidf_text_")) %>% 
  arrange(estimate) %>% 
  view()
```


### xgb model

```{r}

```


### Nearest neighbour classification

What I want to do is return the most similar car record to the particular 

We can do this with clustering perhaps.

```{r}
library(embed)

df_small <- df %>%
  drop_na() %>% 
  slice_sample(n = 1000000) %>% 
  mutate(id = row_number())

umap_rec <- recipe(~., data = df_small) %>%
  update_role(c(id, title, text, ad_url), new_role = "id") %>%
  step_other(all_nominal_predictors(), threshold = .01) %>%
  step_zv(all_predictors()) %>%
  step_novel(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors(), one_hot = T) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_umap(all_predictors(), num_comp = 2)

umap_juiced <- prep(umap_rec) %>% juice()

umap_juiced
```


```{r}
umap_juiced %>%
  ggplot(aes(UMAP1, UMAP2, label = title)) +
  geom_point(alpha = 0.7, size = 2) +
  geom_text(check_overlap = TRUE, hjust = "inward", family = "IBMPlexSans") +
  labs(color = NULL)
```


```{r}
umap_juiced

points <- umap_juiced %>% 
  slice(1) %>% 
  select(UMAP1, UMAP2) %>% 
  rename(p1 = UMAP1, p2 = UMAP2)

dist( method = "euclidean")

find_similar <- function(datapoint){
  umap_juiced %>% 
    bind_cols(points) %>% 
    mutate(dist = sqrt(sum(UMAP1 - p1)^2+sum(UMAP2 - p2)^2))
  
}
```


```{r}
x1 <- rnorm(30)
x2 <- rnorm(30)
dist(rbind(x1, x2))

dist(rbind(x1, x2))

euc.dist <- function(x1, x2) sqrt(sum((x1 - x2) ^ 2))

```

### rf model

```{r}
# usemodels::use_ranger(price ~ ., data = df)

ranger_recipe <- 
  recipe(formula = price ~ ., data = df_small) %>% 
  update_role(c(title), new_role = "id") %>%
  step_other(all_nominal_predictors(), threshold = .01) %>%
  step_zv(all_predictors()) %>%
  step_novel(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors())

ranger_spec <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_mode("regression") %>% 
  set_engine("ranger") 

rf_param <-
  ranger_spec %>%
  extract_parameter_set_dials() %>%
  finalize(ranger_recipe %>% prep() %>% juice())

ranger_grid <- grid_regular(rf_param, levels = 5)

ranger_workflow <- 
  workflow() %>% 
  add_recipe(ranger_recipe) %>% 
  add_model(ranger_spec) 

df_folds <- vfold_cv(df_small, v = 5)

set.seed(35807)
ranger_tune <- tune_grid(ranger_workflow, resamples = df_folds, grid = ranger_grid)
```


```{r}
ranger_tune %>% 
  autoplot()

ranger_workflow_final <- ranger_workflow %>% 
  finalize_workflow(select_best(ranger_tune, "rsq"))
```

```{r}

```

### MARS model

purpose of the mars model is to get some non-linearity into the model

```{r}
# try with small set

df_smaller <- df_small %>% 
  slice_sample(n = 10000)

df_folds_small <- vfold_cv(df_smaller, v = 5)

earth_recipe <-
  recipe(formula = price ~ make_model + kilometers + year + lat + long, data = df_smaller) %>%
  step_other(make_model, threshold = .01) %>%
  step_novel(make_model) %>%
  step_interact(terms = ~ kilometers:year) %>%
  step_interact(terms = ~ lat:long) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors())

earth_spec <- 
  mars(num_terms = tune(), prod_degree = tune(), prune_method = "none") %>% 
  set_mode("regression") %>% 
  set_engine("earth") 

earth_workflow <- 
  workflow() %>% 
  add_recipe(earth_recipe) %>% 
  add_model(earth_spec) 

earth_grid <- tidyr::crossing(num_terms = 2 * (1:6), prod_degree = 1:2) 

earth_tune <- 
  tune_grid(earth_workflow, resamples = df_folds_small, grid = earth_grid) 

earth_tune %>% 
  show_best()
  collect_metrics()
```






